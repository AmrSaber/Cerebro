\subsection{architecture}
    We represent the project to be a library so that if any other project needs to use any functionality we support they can do so easily by just downloading and importing our library.
\bigbreak
    The library can be used by the higher level APIs or 
    lower level methods and its structure is as follows :
        \begin{itemize}
             \item Emotion Recognition Interface 
                \begin{itemize}
                    \item Real Time Streaming
                    \item Video
                    \item Single Image
                \end{itemize}
             \item Lower Level APIs
                \begin{itemize}
                    \item Model API
                    \item Feature Extraction API
                    \item Image Processing API
                \end{itemize}
        \end{itemize}

\subsubsection{Real Time Streaming}
The Real Time Stream interface is developed in a way that works with threads so that the stream running smoothly with high efficiency with no noticed delay in processing the frames to recognize the emotions 
\bigbreak 
\noindent To use the stream function :
% indentation of verbatim must be like that please don't change it
\begin{itemize}
\item import the module:
\begin{verbatim}
 from interface Import video-stream as vs
\end{verbatim}
\item call it:
\begin{verbatim}
vs.detect_stream_emotions(fps)
\end{verbatim}
\end{itemize}
The frame-per-second value is an integer represents the number of frames will be processed to recognize emotion in per one second. default sit to be 40.
\bigbreak

\subsubsection{Single Image Interface}

The Single Image Interface takes only one image as input and return image
with emotions recognized 

\noindent To use the Single Image Interface 
\begin{itemize}
\item import the module:
\begin{verbatim}
from interface Import process_image as pi
\end{verbatim}
\item call it:
\begin{verbatim}
pi.mark_faces_emotions(image)
\end{verbatim}
\end{itemize}
\newpage

\subsection{Processing video}
It works by calling detect\_video\_emotion from process\_video.py.
\newline \textbf{\textit{Input}}: 
\begin{itemize}
\item  \textbf{video\_path}: the video which supposed to be processed.
\item  \textbf{output\_path}: output video path, should contain output video name with its extension (must be mp4).
\item  \textbf{skip}: how many frames you want it to be skipped without emotion detection, be careful by increasing this parameter the output would be less accurate but the processing would be faster, its default value is 50 and that mean detect emotion every 2 seconds in normal video speed.
\item \textbf{detector\_type}: the default is dlib detector but it can be controlled to be HAAR or LBP classifiers.
\item  \textbf{verbose}: to follow processing operation by some print line this should be true, its default value is false.
\end{itemize}
\textbf{\textit{Output}}: it doesn't return anything as its output directly saved to the output path.
\newline\textbf{\textit{ Rule}}: it divides the video into frames and audio, applies analysis on it then gathers them again.
\newline 
